## Practical Work 2 (PW2, Individual): Combining multiple classifiers

## Installation
In order to run the program, first of all we need to install packages.

run the following command:
`pip install -r requirements.txt`

## Usage
you should visit main file to run all algorithms and analyses



## Procedure

1. Implement a Random Forest and a Decision Forest technique.
The base-learner for inducing the trees will be the CART method.

    a. Both the Random Forest (RF) and the Decision Forest (DF) classifier
    must be able to read a dataset in csv file format

    b. Learn the model (the random forest or decision forest) from the 
    training data set, and at the same time produce an ordered list of 
    the features used in the forest, according to its importance. The 
    importance can be estimated as the frequency of its appearance in 
    the random forest/random decision constructed.

    c. The models must have, at least, the hyper-parameter F (number of 
    random features used in the splitting of the nodes in RF or in each 
    tree in DF) and the number of trees (NT) desired.


2. Forest interpreter implemented, with that, is given a random forest 
or a decision forest would be able to classify a test dataset, obtaining 
the corresponding classification accuracy values (or generalization error) 
for different combination of values of F and NT. For instance, try (when 
make sense), being M the total number of features:


    Random Forest
    â€¢ Each training set for each tree is a bootstrapped sampling of the 
    original training set
    â€¢ NT=1,10,25,50,75,100
    â€¢ F=1,3,ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(log2ğ‘€ğ‘€+1),âˆšğ‘€ğ‘€
    Decision Forest
    â€¢ Each training set for each tree is the same original training set
    â€¢ NT=1,10,25,50,75,100
    â€¢ F = ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘€ğ‘€ï¿½4) , ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘€ğ‘€ï¿½2) , ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(3âˆ—ğ‘€ğ‘€ï¿½4), ğ‘…ğ‘…ğ‘…ğ‘…ğ‘–ğ‘–ğ‘–ğ‘–ğ‘…ğ‘…(1,M) for each tree)

